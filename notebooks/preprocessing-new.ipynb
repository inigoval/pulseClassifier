{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Average Values\n",
    "\n",
    "Create a table filled with average values for heuristics of each class.\n",
    "\n",
    "\n",
    "### Labels\n",
    "\n",
    "#### RFI\n",
    "\n",
    "* 1 : Unclipped RFI/Noise\n",
    "* 2 : Wide-band, long-duration in time clipped RFI (2016+)\n",
    "* 3 : Wide-band, short-duration in time clipped RFI (2016+)\n",
    "* 4 : Wide-band, short duration clipped RFI (2015)\n",
    "* 5 : Sharp bandpass transition\n",
    "* 6 : Wide-band, bursty clipped RFI (2015)\n",
    "* 7 : Error in spectra captue or replacement\n",
    "* 8 : Systematic int/float overflow\n",
    "\n",
    "#### Astronomical\n",
    "\n",
    "* 0 : Interesting, follow up\n",
    "* 9 : Known Pulsar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "Read in the dataframe and prepare the features for use in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.22.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('featureDataframe1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             datfile                     filterbank\n",
      "0      Beam1_dm_D20171125T005835.dat  Beam4_fb_D20171115T020611.fil\n",
      "1      Beam1_dm_D20171128T235111.dat  Beam4_fb_D20171115T020611.fil\n",
      "2      Beam1_dm_D20171128T235111.dat  Beam1_fb_D20171128T235111.fil\n",
      "3      Beam1_dm_D20171126T150911.dat  Beam2_fb_D20171115T154117.fil\n",
      "4      Beam0_dm_D20171115T020611.dat  Beam2_fb_D20171128T235111.fil\n",
      "5      Beam1_dm_D20171121T002911.dat  Beam2_fb_D20171128T235111.fil\n",
      "6      Beam1_dm_D20171121T002911.dat  Beam1_fb_D20171121T002911.fil\n",
      "7      Beam1_dm_D20171121T002911.dat  Beam2_fb_D20171128T235111.fil\n",
      "8      Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171117T004412.fil\n",
      "9      Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "10     Beam1_dm_D20171121T002911.dat  Beam2_fb_D20171128T235111.fil\n",
      "11     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171117T004412.fil\n",
      "12     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171117T004412.fil\n",
      "13     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171121T002911.fil\n",
      "14     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171121T002911.fil\n",
      "15     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171121T002911.fil\n",
      "16     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "17     Beam1_dm_D20171121T002911.dat  Beam2_fb_D20171128T235111.fil\n",
      "18     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "19     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171121T002911.fil\n",
      "20     Beam1_dm_D20171121T002911.dat  Beam1_fb_D20171121T002911.fil\n",
      "21     Beam1_dm_D20171121T002911.dat  Beam1_fb_D20171121T002911.fil\n",
      "22     Beam1_dm_D20171121T002911.dat  Beam1_fb_D20171121T002911.fil\n",
      "23     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "24     Beam1_dm_D20171121T002911.dat  Beam1_fb_D20171121T002911.fil\n",
      "25     Beam1_dm_D20171121T002911.dat  Beam6_fb_D20171121T002911.fil\n",
      "26     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "27     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "28     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171121T002911.fil\n",
      "29     Beam1_dm_D20171121T002911.dat  Beam4_fb_D20171130T203528.fil\n",
      "...                              ...                            ...\n",
      "35602  Beam6_dm_D20160614T170104.dat  Beam6_fb_D20160614T170104.fil\n",
      "35603  Beam6_dm_D20161118T074503.dat  Beam6_fb_D20161118T074503.fil\n",
      "35604  Beam6_dm_D20161119T002404.dat  Beam6_fb_D20161119T002404.fil\n",
      "35605  Beam6_dm_D20161119T002404.dat  Beam6_fb_D20161119T002404.fil\n",
      "35606  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35607  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35608  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35609  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35610  Beam6_dm_D20160827T050203.dat  Beam6_fb_D20160827T050203.fil\n",
      "35611  Beam6_dm_D20160827T050203.dat  Beam6_fb_D20160827T050203.fil\n",
      "35612  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35613  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35614  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35615  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35616  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35617  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35618  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35619  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35620  Beam6_dm_D20161015T094603.dat  Beam6_fb_D20161015T094603.fil\n",
      "35621  Beam6_dm_D20160825T161004.dat  Beam6_fb_D20160825T161004.fil\n",
      "35622  Beam6_dm_D20160203T162304.dat  Beam6_fb_D20160203T162304.fil\n",
      "35623  Beam6_dm_D20160203T162304.dat  Beam6_fb_D20160203T162304.fil\n",
      "35624  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35625  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35626  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35627  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35628  Beam6_dm_D20161010T200104.dat  Beam6_fb_D20161010T200104.fil\n",
      "35629  Beam6_dm_D20160203T024904.dat  Beam6_fb_D20160203T024904.fil\n",
      "35630  Beam6_dm_D20160203T024904.dat  Beam6_fb_D20160203T024904.fil\n",
      "35631  Beam6_dm_D20160129T000103.dat  Beam6_fb_D20160129T000103.fil\n",
      "\n",
      "[125439 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.describe()\n",
    "\n",
    "print df[['datfile', 'filterbank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125439, 489)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-label Mis-labelled Type 8 Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reLabel = [('Beam4_fb_D20150821T195709.fil', 202),\n",
    "           ('Beam0_fb_D20150906T194609.fil', 275),\n",
    "           ('Beam4_fb_D20150909T235709.fil', 467),\n",
    "           ('Beam6_fb_D20150925T221909.fil', 3),\n",
    "           ('Beam1_fb_D20151215T221703.fil', 377),\n",
    "           ('Beam1_fb_D20151215T221703.fil', 595),\n",
    "           ('Beam1_fb_D20151216T231103.fil', 50),\n",
    "           ('Beam1_fb_D20151217T050412.fil', 196),\n",
    "           ('Beam1_fb_D20151217T050412.fil', 197),\n",
    "           ('Beam1_fb_D20151217T050412.fil', 202),\n",
    "           ('Beam0_fb_D20160824T104004.fil', 1392),\n",
    "           ('Beam0_fb_D20160825T003010.fil', 3),\n",
    "           ('Beam0_fb_D20160824T104004.fil', 1392),\n",
    "           ('Beam0_fb_D20160825T003010.fil', 3),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 6),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 8),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 9),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 11),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 14),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 15),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 16),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 17),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 19),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 21),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 25),\n",
    "           ('Beam1_fb_D20170310T020409.fil', 4),\n",
    "           ('Beam4_fb_D20170310T020409.fil', 7),\n",
    "           ('Beam5_fb_D20170310T020409.fil', 1),\n",
    "           ('Beam5_fb_D20170310T020409.fil', 2),\n",
    "           ('Beam6_fb_D20170311T184903.fil', 2),\n",
    "           ('Beam1_fb_D20170312T182504.fil', 2),\n",
    "           ('Beam2_fb_D20170325T011509.fil', 1),\n",
    "           ('Beam1_fb_D20170310T020409.fil', 3),\n",
    "           ('Beam0_fb_D20170311T184903.fil', 2),\n",
    "           ('Beam1_fb_D20170311T184903.fil', 1),\n",
    "           ('Beam5_fb_D20170312T182504.fil', 1),\n",
    "           ('Beam1_fb_D20170310T020409.fil', 2),\n",
    "           ('Beam6_fb_D20170311T184903.fil', 3),\n",
    "           ('Beam4_fb_D20170312T182504.fil', 10),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 2),\n",
    "           ('Beam4_fb_D20170310T020409.fil', 4),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 1),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 3),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 4),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 5),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 7),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 10),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 12),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 13),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 18),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 20),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 22),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 23),\n",
    "           ('Beam0_fb_D20170310T020409.fil', 24),\n",
    "           ('Beam1_fb_D20170310T020409.fil', 1),\n",
    "           ('Beam4_fb_D20170310T020409.fil', 3),\n",
    "           ('Beam4_fb_D20170310T020409.fil', 5),\n",
    "           ('Beam4_fb_D20170310T020409.fil', 6),\n",
    "           ('Beam0_fb_D20170311T184903.fil', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relabel mis-labelled events to overflow (label 8)\n",
    "for fil,buf in reLabel:\n",
    "    #print fil, buf\n",
    "    df.loc[(df['filterbank']==fil) & (df['Buffer']==buf), 'Label'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# array = df.values.astype('float32')\n",
    "# args = np.argwhere(array>1e+38)\n",
    "# idx = np.hsplit(args,2)\n",
    "# idx = idx.flatten()\n",
    "# df.drop(index=idx, inplace=True)\n",
    "\n",
    "# print df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MJD Features\n",
    "\n",
    "We don't care about the absolute MJDs, but the relative offset within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['relMJDmax'] = df['MJDmax'] - df['MJDmin']\n",
    "df['relMJDmean'] = df['MJDmean'] - df['MJDmin']\n",
    "df['relMJDmedian'] = df['MJDmedian'] - df['MJDmin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop extremely large values (overflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# #Create a dataframe with only float values, convert to a numpy array:\n",
    "# floatdf = df.drop(['Beam', 'Buffer', 'TSID', 'Label', 'datfile', 'predictLabel', 'filterbank', 'MJDmax', 'MJDmean', 'MJDmedian', 'MJDmin'], axis=1)\n",
    "# fullarr = floatdf.values.astype('float32')\n",
    "\n",
    "# #Find row indices where values are extremely large:\n",
    "# overflowargs = np.argwhere(fullarr > 1e+7)\n",
    "# overflowidx = np.hsplit(overflowargs,2)[0]\n",
    "# overflowidx = np.unique(overflowidx)\n",
    "# overflowidx.flatten()\n",
    "# print overflowidx.shape\n",
    "\n",
    "# #Put overflows in label 8:\n",
    "# for i in overflowidx:\n",
    "#     df.ix[i, 'Label'] = 8\n",
    "#     #print i\n",
    "\n",
    "# #Drop overflow values:\n",
    "# fullar = df.drop(overflowidx)\n",
    "\n",
    "# fullarr = floatdf.values.astype('float32')\n",
    "# print np.amax(fullarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel overflows as class 8 and split dataframe into labelled and unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    idx, data = row\n",
    "    if data['ofCount'] > 0.: \n",
    "        df.ix[idx, 'Label'] = 8\n",
    "        #print idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             datfile                     filterbank\n",
      "83     Beam1_dm_D20170325T011509.dat  Beam1_fb_D20170325T011509.fil\n",
      "84     Beam1_dm_D20170325T011509.dat  Beam1_fb_D20170325T011509.fil\n",
      "85     Beam1_dm_D20170325T092403.dat  Beam1_fb_D20170325T092403.fil\n",
      "94     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "95     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "96     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "97     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "98     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "99     Beam1_dm_D20170324T000404.dat  Beam1_fb_D20170324T000404.fil\n",
      "104    Beam1_dm_D20170320T173204.dat  Beam1_fb_D20170320T173204.fil\n",
      "105    Beam1_dm_D20170320T173204.dat  Beam1_fb_D20170320T173204.fil\n",
      "106    Beam1_dm_D20170320T173204.dat  Beam1_fb_D20170320T173204.fil\n",
      "108    Beam1_dm_D20170331T004609.dat  Beam6_fb_D20170324T215304.fil\n",
      "110    Beam1_dm_D20170325T033303.dat  Beam1_fb_D20170325T033303.fil\n",
      "113    Beam1_dm_D20170325T033303.dat  Beam1_fb_D20170325T033303.fil\n",
      "116    Beam1_dm_D20170321T083204.dat  Beam1_fb_D20170321T083204.fil\n",
      "117    Beam1_dm_D20170321T083204.dat  Beam1_fb_D20170321T083204.fil\n",
      "118    Beam1_dm_D20170321T083204.dat  Beam1_fb_D20170321T083204.fil\n",
      "120    Beam1_dm_D20170321T083204.dat  Beam1_fb_D20170321T083204.fil\n",
      "121    Beam0_dm_D20170320T204704.dat  Beam6_fb_D20170322T001109.fil\n",
      "122    Beam1_dm_D20170325T010003.dat  Beam6_fb_D20170322T001109.fil\n",
      "123    Beam1_dm_D20170325T010003.dat  Beam1_fb_D20170325T010003.fil\n",
      "124    Beam0_dm_D20170322T233609.dat  Beam0_fb_D20170322T233609.fil\n",
      "125    Beam0_dm_D20170328T054808.dat  Beam6_fb_D20170322T001109.fil\n",
      "173    Beam0_dm_D20170321T035204.dat  Beam0_fb_D20170321T035204.fil\n",
      "176    Beam1_dm_D20170321T184704.dat  Beam1_fb_D20170321T184704.fil\n",
      "177    Beam0_dm_D20170322T000504.dat  Beam0_fb_D20170322T000504.fil\n",
      "178    Beam0_dm_D20170322T000504.dat  Beam0_fb_D20170322T000504.fil\n",
      "179    Beam0_dm_D20170320T173204.dat  Beam0_fb_D20170320T173204.fil\n",
      "180    Beam0_dm_D20170320T173204.dat  Beam0_fb_D20170320T173204.fil\n",
      "...                              ...                            ...\n",
      "35601  Beam6_dm_D20160614T170104.dat  Beam6_fb_D20160614T170104.fil\n",
      "35602  Beam6_dm_D20160614T170104.dat  Beam6_fb_D20160614T170104.fil\n",
      "35603  Beam6_dm_D20161118T074503.dat  Beam6_fb_D20161118T074503.fil\n",
      "35604  Beam6_dm_D20161119T002404.dat  Beam6_fb_D20161119T002404.fil\n",
      "35605  Beam6_dm_D20161119T002404.dat  Beam6_fb_D20161119T002404.fil\n",
      "35606  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35607  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35608  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35609  Beam6_dm_D20160819T181904.dat  Beam6_fb_D20160819T181904.fil\n",
      "35610  Beam6_dm_D20160827T050203.dat  Beam6_fb_D20160827T050203.fil\n",
      "35611  Beam6_dm_D20160827T050203.dat  Beam6_fb_D20160827T050203.fil\n",
      "35612  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35613  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35614  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35615  Beam6_dm_D20161119T000203.dat  Beam6_fb_D20161119T000203.fil\n",
      "35616  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35617  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35619  Beam6_dm_D20160124T033009.dat  Beam6_fb_D20160124T033009.fil\n",
      "35620  Beam6_dm_D20161015T094603.dat  Beam6_fb_D20161015T094603.fil\n",
      "35621  Beam6_dm_D20160825T161004.dat  Beam6_fb_D20160825T161004.fil\n",
      "35622  Beam6_dm_D20160203T162304.dat  Beam6_fb_D20160203T162304.fil\n",
      "35623  Beam6_dm_D20160203T162304.dat  Beam6_fb_D20160203T162304.fil\n",
      "35624  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35625  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35626  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35627  Beam6_dm_D20161012T162904.dat  Beam6_fb_D20161012T162904.fil\n",
      "35628  Beam6_dm_D20161010T200104.dat  Beam6_fb_D20161010T200104.fil\n",
      "35629  Beam6_dm_D20160203T024904.dat  Beam6_fb_D20160203T024904.fil\n",
      "35630  Beam6_dm_D20160203T024904.dat  Beam6_fb_D20160203T024904.fil\n",
      "35631  Beam6_dm_D20160129T000103.dat  Beam6_fb_D20160129T000103.fil\n",
      "\n",
      "[12398 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "labelledDf = df[df['Label'] > -1]\n",
    "labelledDf = labelledDf[labelledDf['Label'] != 8]\n",
    "\n",
    "#Merge classes 9 and 0:\n",
    "labelledDf.loc[labelledDf['Label'] == 0, 'Label'] = 9\n",
    "\n",
    "unlabelledDf = df[df['Label'] < 0.5]\n",
    "\n",
    "#print unlabelledDf[['datfile', 'filterbank']]\n",
    "print labelledDf[['datfile', 'filterbank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in labelledDf:\n",
    "    if labelledDf[col].dtype == object:\n",
    "        print col, labelledDf[col].min(), labelledDf[col].max() \n",
    "    else: \n",
    "        print col, labelledDf[col].min(), labelledDf[col].max(), labelledDf[col].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Direct Data Cleaning Filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that data during March 2017 has been removed\n",
    "# for row in labelledDf.iterrows():\n",
    "#     idx, data = row\n",
    "#     if (data['globalTimeStatsmean'] < -3e3) or (data['globalTimeStatsmin'] < -3e3): \n",
    "#         print idx\n",
    "\n",
    "# for row in unlabelledDf.iterrows():\n",
    "#     idx, data = row\n",
    "#     if (data['globalTimeStatsmean'] < -3e3) or (data['globalTimeStatsmin'] < -3e3): \n",
    "#         print idx\n",
    "        \n",
    "# Check that all events which overflow have been dropped\n",
    "# count = 0 \n",
    "# for row in labelledDf.iterrows():\n",
    "#     idx, data = row\n",
    "#     if data['ofCount'] > 0.: \n",
    "#         #print idx\n",
    "#         count += 1\n",
    "# print count\n",
    "        \n",
    "# count = 0 \n",
    "# for row in unlabelledDf.iterrows():\n",
    "#     idx, data = row\n",
    "#     if data['ofCount'] > 0.: \n",
    "#         #print idx\n",
    "#         count += 1\n",
    "# print count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(n timeseries, m features)\n",
    "print labelledDf.shape\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select out the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labelledDf['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the meta-information columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelledDf.columns #list column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuredf = labelledDf.drop(['Beam', 'Buffer', 'datfile', 'TSID', 'Label', 'predictLabel', 'filterbank', 'MJDmax', 'MJDmean', 'MJDmedian', 'MJDmin'], axis=1)\n",
    "unlblfeaturedf = unlabelledDf.drop(['Beam', 'Buffer', 'datfile', 'TSID', 'Label', 'predictLabel', 'filterbank', 'MJDmax', 'MJDmean', 'MJDmedian', 'MJDmin'], axis=1)\n",
    "fulldf = df.drop(['Beam', 'Buffer', 'TSID', 'Label', 'datfile', 'predictLabel', 'filterbank', 'MJDmax', 'MJDmean', 'MJDmedian', 'MJDmin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featuredf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Prepare the features for model building: outliers (inf, NaN) need to be replaces, the features need to be scaled to zero mean, and unity variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert feature dataframe to 2-D array of floats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresArr = featuredf.values.astype('float32')\n",
    "unlblfeaturesArr = unlblfeaturedf.values.astype('float32')\n",
    "fullarr = fulldf.values.astype('float32')\n",
    "preProcLabels = labels.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace NaN and inf values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresArr[featuresArr == -np.inf] = np.nan\n",
    "featuresArr[featuresArr == np.inf] = np.nan #set infinite values to nan (could u use .isinf more easily here?)\n",
    "print featuresArr[featuresArr == np.nan]\n",
    "\n",
    "unlblfeaturesArr[unlblfeaturesArr == -1. * np.inf] = np.nan\n",
    "unlblfeaturesArr[unlblfeaturesArr == np.inf] = np.nan\n",
    "print unlblfeaturesArr[unlblfeaturesArr == np.nan]\n",
    "\n",
    "fullarr[fullarr == -1. * np.inf] = np.nan\n",
    "fullarr[fullarr == np.inf] = np.nan\n",
    "print fullarr[fullarr == np.nan]\n",
    "\n",
    "#replace missing values with the mean along axis 0, np.nan values replaced with 'NaN' string\n",
    "#imp = sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0).fit(featuresArr)\n",
    "#unlblimp = sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0).fit(unlblfeaturesArr)\n",
    "\n",
    "imp1 = sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0).fit(featuresArr)\n",
    "preProcFeats = imp1.transform(featuresArr)\n",
    "\n",
    "imp2 = sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0).fit(unlblfeaturesArr)\n",
    "unlblpreProcFeats = imp2.transform(unlblfeaturesArr)\n",
    "\n",
    "imp3 = sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0).fit(fullarr)\n",
    "fullfeats = imp3.transform(fullarr)\n",
    "\n",
    "print np.argwhere(np.isfinite(preProcFeats) != True)\n",
    "print np.argwhere(np.isfinite(fullfeats) != True)\n",
    "\n",
    "print preProcFeats.shape, fullfeats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace values very close to 0 with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# args = np.isclose(fullfeats, 0.)\n",
    "# fullfeats[args] = 0\n",
    "\n",
    "# args = np.isclose(unlblpreProcFeats, 0.)\n",
    "# unlblpreProcFeats[args] = 0\n",
    "\n",
    "# args = np.isclose(preProcFeats, 0.)\n",
    "# preProcFeats[args] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missed overflow values (class 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unlabelledDf = unlabelledDf.reset_index(drop=True)\n",
    "# unlblfeaturedf = unlblfeaturedf.reset_index(drop=True)\n",
    "\n",
    "# ofargs = np.argwhere(preProcFeats > 1e+38)\n",
    "# ofidx = np.hsplit(ofargs,2)[0]\n",
    "# ofidx = ofidx.flatten()\n",
    "# preProcFeats = np.delete(preProcFeats, ofidx, axis=0)\n",
    "# preProcLabels = np.delete(preProcLabels, ofidx, axis=0)\n",
    "# labelledDf = labelledDf.drop(labelledDf.index[ofidx])\n",
    "# featuredf = featuredf.drop(featuredf.index[ofidx])\n",
    "\n",
    "\n",
    "# ofargs = np.argwhere(preProcFeats < -1e+38)\n",
    "# ofidx = np.hsplit(ofargs,2)[0]\n",
    "# ofidx = ofidx.flatten()\n",
    "# preProcFeats = np.delete(preProcFeats, ofidx, axis=0)\n",
    "# preProcLabels = np.delete(preProcLabels, ofidx, axis=0)\n",
    "# labelledDf = labelledDf.drop(labelledDf.index[ofidx])\n",
    "# featuredf = featuredf.drop(featuredf.index[ofidx])\n",
    "\n",
    "# #print unlabelledDf.shape, unlabelledDf.shape\n",
    "# #print unlabelledDf.index, unlabelledDf.index\n",
    "\n",
    "# ofargs = np.argwhere(unlblpreProcFeats > 1e+38)\n",
    "# ofidx = np.hsplit(ofargs,2)[0]\n",
    "# ofidx = ofidx.flatten()\n",
    "\n",
    "# unlblpreProcFeats = np.delete(unlblpreProcFeats, ofidx, axis=0)\n",
    "# unlblfeaturedf = unlblfeaturedf.drop(unlabelledDf.index[ofidx])\n",
    "# unlabelledDf = unlabelledDf.drop(ofidx)\n",
    "\n",
    "# #print unlabelledDf.shape, unlblpreProcFeats.shape\n",
    "\n",
    "# ofargs = np.argwhere(unlblpreProcFeats < -1e+38)\n",
    "# ofidx = np.hsplit(ofargs,2)[0]\n",
    "# ofidx = ofidx.flatten()\n",
    "# unlblpreProcFeats = np.delete(unlblpreProcFeats, ofidx, axis=0)\n",
    "# unlblfeaturedf = unlblfeaturedf.drop(labels = ofidx,axis=0)\n",
    "# unlabelledDf = unlabelledDf.drop(ofidx)\n",
    "\n",
    "# #print unlabelledDf.shape, unlblpreProcFeats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.amax(preProcFeats)\n",
    "print np.amin(preProcFeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale Features**\n",
    "\n",
    "Robust scaling uses a median filter, this is better for non-Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rScale = sklearn.preprocessing.RobustScaler().fit(fullfeats)\n",
    "#print rScale.scale_\n",
    "#print rScale.center_\n",
    "\n",
    "preProcFeats = rScale.transform(preProcFeats)\n",
    "\n",
    "print preProcFeats.shape\n",
    "\n",
    "#print rScale.scale_\n",
    "#print rScale.center_\n",
    "\n",
    "unlblpreProcFeats = rScale.transform(unlblpreProcFeats)\n",
    "\n",
    "print unlblpreProcFeats.shape\n",
    "print preProcFeats.shape, preProcLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for infinite outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infidx = np.argwhere(np.isfinite(preProcFeats) != True)\n",
    "print np.unique(np.hsplit(infidx,2)[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold='nan')\n",
    "#print preProcFeats[infidx]\n",
    "\n",
    "infcol = np.hsplit(infidx,2)[1]\n",
    "infcol = np.unique(infcol)\n",
    "inffeats = featuredf.columns[infcol]\n",
    "print inffeats.shape\n",
    "\n",
    "print inffeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write important dataframes/arrays to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./processed.files/labelled/featuresnew', preProcFeats)\n",
    "np.save('./processed.files/labelled/labelsnew', preProcLabels)\n",
    "featuredf.to_pickle('./processed.files/labelled/featureDfnew')\n",
    "oldfeaturedf.to_pickle('./processed.files/labelled/oldfeaturedf')\n",
    "labelledDf.to_pickle('./processed.files/labelled/labelledDfnew')\n",
    "\n",
    "np.save('./processed.files/raw/featuresnew', unlblpreProcFeats)\n",
    "unlblfeaturedf.to_pickle('./processed.files/raw/featureDfnew')\n",
    "unlabelledDf.to_pickle('./processed.files/raw/unlabelledDfnew')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
