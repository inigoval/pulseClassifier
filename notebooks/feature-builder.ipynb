{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALFABURST Event Buffer Feature Builder\n",
    "\n",
    "The ALFABURST commensal FRB search survey searches for dedisperesed pulses above a signal to noise of 10 across of 56 MHz band. Data is processed in time windows of 2^15 * 256 microseconds (~8.4 seconds), 512 frequency channels. If a pulse is detected the entire time window is recorded to disk.\n",
    "\n",
    "The vast majority of detected pulses are false-positive events due to human-made RFI. Only a small minority of events (less than 1%) is due to astrophysical sources, primarily bright pulses from pulsars. The RFI takes on a wide range of characteristics. In the processing pipeline the brightest RFI is clipped and replaced, but low-level RFI and spectra statistics still lead to an excess of false-positives.\n",
    "\n",
    "In order to automate the processing the 150000+ recorded buffers a classifier model would be useful to ***probabilistically*** classify each event. Approximately 15000 events have been labelled into 10 different categories. We can use this *labelled* data set for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '/data2/alfaburstdata200218/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build buffer database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseBufferPklFile = '/home/inigo/pulseClassifier/notebooks/newdf1.pkl'\n",
    "\n",
    "# load baseBufferPkl\n",
    "df = pd.read_pickle(baseBufferPklFile)\n",
    "\n",
    "# create a predicted label column with 'unlabelled' label\n",
    "df = df.assign(predictLabel=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial buffer dataframe contains a list of all buffers with meta-data such as time, beam ID, and buffer ID. There is also global statistics for each buffer usch as number of events in the buffer and the maximum SNR event. The label column is initially empty, we need to fill it with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Beam         Buffer       MJDstart         bestDM  \\\n",
      "count  125439.000000  125439.000000  125439.000000  125439.000000   \n",
      "mean        3.531262     216.217652   57429.620744    1349.748866   \n",
      "std         2.090338     282.191040     236.046434    2608.874241   \n",
      "min         0.000000       1.000000   57197.378446       0.000000   \n",
      "25%         2.000000      22.000000   57275.185900       9.000000   \n",
      "50%         3.000000     113.000000   57324.388785      16.000000   \n",
      "75%         6.000000     301.000000   57414.833449    1236.000000   \n",
      "max         6.000000    2025.000000   58098.268715   10039.000000   \n",
      "\n",
      "             bestSNR      BinFactor        Events          DMmax  \\\n",
      "count  125439.000000  125439.000000  1.254390e+05  125439.000000   \n",
      "mean       12.915690      14.184632  6.864074e+03    2373.811865   \n",
      "std        60.700000      19.310866  4.732979e+04    3724.165909   \n",
      "min         6.001111       1.000000  1.000000e+00       3.000000   \n",
      "25%        10.541907       2.000000  6.000000e+00      12.000000   \n",
      "50%        11.370401       4.000000  5.200000e+01      38.000000   \n",
      "75%        12.855733      16.000000  7.440000e+02    2559.000000   \n",
      "max     20954.304688      64.000000  2.473715e+06   57287.182376   \n",
      "\n",
      "               DMmin         DMmean      ...             SNRmean  \\\n",
      "count  125439.000000  125439.000000      ...       125439.000000   \n",
      "mean      475.119150    1446.408882      ...           10.690606   \n",
      "std      1376.271409    2404.880171      ...           35.253209   \n",
      "min         0.000000       3.000000      ...            6.001111   \n",
      "25%         5.000000       9.310345      ...           10.247700   \n",
      "50%         8.000000      19.400000      ...           10.449304   \n",
      "75%       155.000000    1718.334871      ...           10.760569   \n",
      "max     10039.000000   10039.000000      ...        12332.098730   \n",
      "\n",
      "           SNRmedian         SNRstd         MJDmax         MJDmin  \\\n",
      "count  125439.000000  113692.000000  125439.000000  125439.000000   \n",
      "mean       10.519629       0.726661   57429.620804   57429.620774   \n",
      "std        29.911673      20.282344     236.046435     236.046428   \n",
      "min         6.001111       0.000006   57197.378488   57197.378447   \n",
      "25%        10.201573       0.236433   57275.185974   57275.185927   \n",
      "50%        10.371506       0.402976   57324.388837   57324.388836   \n",
      "75%        10.610323       0.698678   57414.833497   57414.833465   \n",
      "max     10400.867188    6749.039746   58098.268787   58098.268787   \n",
      "\n",
      "             MJDstd        MJDmean      MJDmedian     Label  predictLabel  \n",
      "count  1.136920e+05  125439.000000  125439.000000  125439.0      125439.0  \n",
      "mean   9.758141e-06   57429.620789   57429.620789      -1.0          -1.0  \n",
      "std    1.090689e-05     236.046432     236.046431       0.0           0.0  \n",
      "min    0.000000e+00   57197.378480   57197.378488      -1.0          -1.0  \n",
      "25%    3.360377e-07   57275.185942   57275.185941      -1.0          -1.0  \n",
      "50%    4.194827e-06   57324.388836   57324.388836      -1.0          -1.0  \n",
      "75%    1.879576e-05   57414.833473   57414.833466      -1.0          -1.0  \n",
      "max    6.888869e-05   58098.268787   58098.268787      -1.0          -1.0  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "['datfile' 'Beam' 'TSID' 'Buffer' 'MJDstart' 'bestDM' 'bestSNR' 'BinFactor'\n",
      " 'Events' 'DMmax' 'DMmin' 'DMmean' 'DMmedian' 'DMstd' 'SNRmean' 'SNRmedian'\n",
      " 'SNRstd' 'MJDmax' 'MJDmin' 'MJDstd' 'MJDmean' 'MJDmedian' 'Label'\n",
      " 'predictLabel']\n"
     ]
    }
   ],
   "source": [
    "print df.describe()\n",
    "print df.columns.values #each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add additional buffer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-be26b94fb2f4>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-be26b94fb2f4>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    if type(key) == str\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# metadata and features pickles\n",
    "picklepath = BASE_DATA_PATH + 'output2/'\n",
    "pickledir = os.listdir(BASE_DATA_PATH + 'output2/')\n",
    "#baseDedispDirs = [BASE_DATA_PATH + 'test/']\n",
    "\n",
    "count = 0\n",
    "\n",
    "for path in pickledir:\n",
    "    \n",
    "    metaPklFns = glob.glob(picklepath + path + '/*.meta.pkl')\n",
    "\n",
    "    if len(metaPklFns) > 0: #if atleast one of these directories exists                             \n",
    "        print 'Found %d files in '%len((metaPklFns)) + picklepath + path #print to confirm existence\n",
    "\n",
    "        for mIdx, metaPkl in enumerate(metaPklFns): #enumerate just pairs an index starting from 0 to each metaPklFns value\n",
    "    \n",
    "            count += 1\n",
    "        \n",
    "            # Event meta-data\n",
    "            baseMetaFn = os.path.basename(metaPkl) #returns last directory in metaPkl pathname (if path ends with '/' returns nothing)\n",
    "            bufID = int(baseMetaFn.split('.')[1].split('buffer')[-1]) #not quite sure about this line? split splits a path into (head, tail)\n",
    "            metaDict = pickle.load(open(metaPkl, 'rb')) #rb = read binary (read pickle file)\n",
    "            idx = df.loc[(df['datfile']==metaDict['dat']) & (df['Buffer']==bufID)].index\n",
    "\n",
    "            df.ix[idx, 'filterbank'] = metaDict['filterbank'] \n",
    "\n",
    "\n",
    "            # Percent of a time series which is 0\n",
    "            #print metaDict\n",
    "            df.ix[idx, 'pctZero'] = metaDict.get('pctZero', 0.)\n",
    "            # take the 0-dm time series derivative, calculate the percent of time series with derivative=0\n",
    "            df.ix[idx, 'pctZeroDeriv'] = metaDict.get('pctZeroDeriv', 0.)\n",
    "\n",
    "\n",
    "            # Overflow counter\n",
    "            # number of values which are above 1e20 threshold\n",
    "            ofDict = metaDict.get('overflows', {'ncount': 0, 'pct': 0.})\n",
    "            df.ix[idx, 'ofCount'] = ofDict['ncount']\n",
    "            df.ix[idx, 'ofPct'] = ofDict['pct']\n",
    "\n",
    "\n",
    "            # Longest continuous run of a constant in the dedispersed time series\n",
    "            # tuple: (maxRun, maxVal, maxRun / float(arr.size))\n",
    "            longestRun = metaDict.get('longestRun', {'maxRun':-1, 'maxVal':-1, 'maxRunpct':-1, \n",
    "                                                   'ddmaxRun':-1, 'ddmaxVal':-1})\n",
    "            for key in longestRun:\n",
    "                df.ix[idx,'longestRun' + key] = longestRun[key]\n",
    "            \n",
    "            \n",
    "#             df.ix[idx, 'longestRun0'] = longestRun[0]\n",
    "#             df.ix[idx, 'longestRun1'] = longestRun[1]\n",
    "#             df.ix[idx, 'longestRun2'] = longestRun[2]\n",
    "\n",
    "\n",
    "            # Global statistics of the DM-0 time series\n",
    "            globalTimeStats = metaDict.get('globalTimeStats', {'std': 0., 'max': 0., 'posCount': 0, \\\n",
    "                                                               'min': 0., 'negPct': 0., 'median': 0.,\\\n",
    "                                                               'meanMedianRatio': 0., 'posPct': 0.,\\\n",
    "                                                               'negCount': 0, 'maxMinRatio': 0.,\\\n",
    "                                                               'mean': 0. }) #returns null values for all metrics if globalTimeStats doesnt exist        \n",
    "            for key in globalTimeStats:\n",
    "                df.ix[idx, 'globalTimeStats' + key] = globalTimeStats[key]\n",
    "\n",
    "\n",
    "\n",
    "            # Global statistics of the best DM time series\n",
    "            globalDedispTimeStats = metaDict.get('globalDedispTimeStats', {'std': 0., 'max': 0., \\\n",
    "                                                               'posCount': 0,\n",
    "                                                               'min': 0., 'negPct': 0., 'median': 0.,\\\n",
    "                                                               'meanMedianRatio': 0., 'posPct': 0.,\\\n",
    "                                                               'negCount': 0, 'maxMinRatio': 0.,\\\n",
    "                                                               'mean': 0. }) \n",
    "            for key in globalDedispTimeStats:\n",
    "                df.ix[idx, 'globalDedispTimeStats' + key] = globalDedispTimeStats[key]\n",
    "\n",
    "\n",
    "            # Statistics of 16 segments of the DM-0 time series\n",
    "            windZeros = np.zeros(16) #empty matrix\n",
    "            windTime = metaDict.get('windTimeStats',{'std':windZeros, 'max':windZeros, \\\n",
    "                                                     'min':windZeros, 'snr':windZeros, \\\n",
    "                                                     'mean':windZeros})\n",
    "            \n",
    "                #key has lower case first letter, previuosly was saved into dataframe with capital first letter so this could fuck up\n",
    "            for key in windTime:\n",
    "                windTimeval = windTime[key]\n",
    "                if windTimeval.size != 1:\n",
    "                    for i in range(16):\n",
    "                        df.ix[idx, 'windTimeStats' + key + str(i)] = windTimeval[i]\n",
    "                else:\n",
    "                    df.ix[idx, 'windTimeStats' + key] = windTimeval\n",
    "                \n",
    "                        \n",
    "\n",
    "            # Statistics of 16 segments of the best DM time series\n",
    "            windDedispTime = metaDict.get('windDedispTimeStats',{'std':windZeros, 'max':windZeros,\\\n",
    "                                                                 'min':windZeros, 'snr':windZeros,\\\n",
    "                                                                 'mean':windZeros})\n",
    "            \n",
    "            for key in windDedispTime:\n",
    "                windDedispTimeval = windDedispTime[key]\n",
    "                if windDedispTime[key].size != 1:\n",
    "                    for i in range(16):\n",
    "                        df.ix[idx, 'windDedispTimeStats' + key + str(i)] = windDedispTime[key][i]  #concatenates each label with its corresponding number\n",
    "                else:\n",
    "                    df.ix[idx, 'windDedispTimeStats' + key] = windDedispTime[key]\n",
    "            \n",
    "            \n",
    "            # Statistics of the coarsely pixelized spectrogram\n",
    "            pixelZeros = np.zeros((16, 4))\n",
    "            pixels = metaDict.get('pixels',{'max':pixelZeros, 'min':pixelZeros, 'mean':pixelZeros})\n",
    "            for i in range(16):\n",
    "                for j in range(4):\n",
    "                    df.ix[idx, 'pixelMax_%i_%i'%(i,j)] = pixels['max'][i][j] \n",
    "                    df.ix[idx, 'pixelMin_%i_%i'%(i,j)] = pixels['min'][i][j]\n",
    "                    df.ix[idx, 'pixelMean_%i_%i'%(i,j)] = pixels['mean'][i][j]\n",
    "\n",
    "            for key in pixels:\n",
    "                if pixels[key].size == 1:\n",
    "                    df.ix[idx, 'pixelstats' + key] = pixels[key]\n",
    "            \n",
    "            # Gaussian testng statistics\n",
    "            GaussianTests = metaDict.get('GaussianTests', { 'kurtosis': 0, 'skew': 0, 'dpearsonomni': 0, \n",
    "                                                         'dpearsonp': 0, 'lsD': 0, 'lsp': 0, 'ks': np.zeros(2)})\n",
    "            for key in GaussianTests:\n",
    "                #print\n",
    "                gausstestval = GaussianTests[key]\n",
    "                if key != 'ks':\n",
    "                    df.ix[idx, 'GaussianTests' + key] = gausstestval\n",
    "                else:\n",
    "                    for i in range(2):\n",
    "                        df.ix[idx, 'GaussianTests' + key + str(i)] = gausstestval[i]\n",
    "\n",
    "\n",
    "            #Segmented Gaussian testing statistics\n",
    "            segGaussianTests = metaDict.get('segGaussianTests', { 'lillieforsmaxp': 0, 'lillieforsmaxD': 0, \n",
    "                                                               'lfDmin': 0, 'lillieforssum': 0, 'dpearson': np.empty([8,2])\n",
    "                                                               , 'dpearsonomnisum': 0, 'dpearsonpsum': 0})\n",
    "            for key in segGaussianTests:\n",
    "                seggaussval = segGaussianTests[key]\n",
    "                if key != 'dpearson': \n",
    "                    df.ix[idx, 'segGaussianTests' + key] = segGaussianTests[key]\n",
    "                else:\n",
    "                    for i in range(8):\n",
    "                        for j in range(2):\n",
    "                            df.ix[idx, 'segGaussianTests' + key + str(i) + str(j)] = seggaussval[i][j]\n",
    "\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                print '%d loops completed' %(count)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print df['pixelMin_1_0'].dropna()\n",
    "#print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output of labelImg2.py\n",
    "labelPKlFiles = glob.glob('/data2/griffin/ALFABURST/allLabels/*.pkl')\n",
    "\n",
    "# add assigned labels to main dataframe\n",
    "for lPkl in labelPKlFiles:\n",
    "    print 'Reading labels from', lPkl\n",
    "    labelDict = pickle.load(open(lPkl, 'rb'))\n",
    "    for key,val in labelDict.iteritems():\n",
    "        fbFN = key.split('buffer')[0] + 'fil'\n",
    "        bufID = int(key.split('.')[1].split('buffer')[-1])\n",
    "        df.loc[(df['filterbank']==fbFN) & (df['Buffer']==bufID), 'Label'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df['Label'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save combined dataframe to file\n",
    "\n",
    "This would be a good point to split into a new notebook as the previous setups have been run to combine the various labels and features into a single dataframe. We will likely not need to re-run this code often, and as it takes a few minutes to run we can just save the final dataframe to file. Then use that dataframe as the starting point for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('featureDataframe1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
